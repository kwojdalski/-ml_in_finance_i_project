{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f3e0a2",
   "metadata": {},
   "source": [
    "predicting the return of a stock in the US market using historical data over a recent period of 20 days\n",
    "\n",
    "The one-day return of a stock :\n",
    "\n",
    "$R^t$ =  $\\frac{P_j^t}{P_j^{t-1}}$ - 1\n",
    "\n",
    "the goal is to find a model which beat the benchmark model's accuracy_score = 0.5131\n",
    "\n",
    "Baseline Model's were only run on the train_set \n",
    "\n",
    "The  Model Report score (Accuracy) with preformed model (on the test_set for tunned models): \n",
    "    Decison tree baseline model : 0.510 (only done on the train_set)\n",
    "    Decison tree tunned model : 0.5325 (Cross validation mean accuracy 0.52)\n",
    "    Xgboost baseline model : 0.53  (Cross validation mean accuracy 0.53 ) (only done on the train_set)\n",
    "    Xgboost tunned model : 0.8775  (Cross validation mean accuracy 0.54) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7623b29",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "3 datasets are provided as csv files, split between training inputs and outputs, and test inputs.\n",
    "\n",
    "Input datasets comprise 47 columns: the first ID column contains unique row identifiers while the other 46 descriptive features correspond to:\n",
    "\n",
    "* **DATE**: an index of the date (the dates are randomized and anonymized so there is no continuity or link between any dates),\n",
    "* **STOCK**: an index of the stock,\n",
    "* **INDUSTRY**: an index of the stock industry domain (e.g., aeronautic, IT, oil company),\n",
    "* **INDUSTRY_GROUP**: an index of the group industry,\n",
    "* **SUB_INDUSTRY**: a lower level index of the industry,\n",
    "* **SECTOR**: an index of the work sector,\n",
    "* **RET_1 to RET_20**: the historical residual returns among the last 20 days (i.e., RET_1 is the return of the previous day and so on),\n",
    "* **VOLUME_1 to VOLUME_20**: the historical relative volume traded among the last 20 days (i.e., VOLUME_1 is the relative volume of the previous day and so on),\n",
    "\n",
    "Output datasets are only composed of 2 columns:\n",
    "\n",
    "* **ID**: the unique row identifier (corresponding to the input identifiers)\n",
    "and the binary target:\n",
    "* **RET**: the sign of the residual stock return at time $t$\n",
    "\n",
    "The solution files submitted by participants shall follow this output dataset format (i.e contain only two columns, ID and RET, where the ID values correspond to the input test data). \n",
    "An example submission file containing random predictions is provided.\n",
    "\n",
    "**418595 observations (i.e. lines) are available for the training datasets while 198429 observations are used for the test datasets.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa4def",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3180fc",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29c13d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"./data/x_train.csv\")\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\")\n",
    "train_df = pd.concat([x_train, y_train], axis=1)\n",
    "test_df = pd.read_csv(\"./data/x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e31b6a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "target = 'RET'\n",
    "IDcol = ['ID','STOCK', 'DATE','INDUSTRY','INDUSTRY_GROUP','SECTOR','SUB_INDUSTRY']\n",
    "kfold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a056d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "train_df = train_df.drop(IDcol, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna()\n",
    "test_df = test_df.drop(IDcol, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098490e",
   "metadata": {},
   "source": [
    "**Creat new parameters to increase the accurancy:**\n",
    "- mean return of last **20** days\n",
    "- the moving average of last **5** days\n",
    "- the moving average of last **10** days\n",
    "- the moving average of last **15** days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13016d06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_df['Mean'] = train_df[[f'RET_{i}' for i in range(1, 21)]].mean(axis=1)\n",
    "test_df['Mean'] = test_df[[f'RET_{i}' for i in range(1, 21)]].mean(axis=1)\n",
    "train_df['MA5'] = train_df[[f'RET_{i}' for i in range(1, 7)]].mean(axis=1)\n",
    "train_df['MA10'] = train_df[[f'RET_{i}' for i in range(1, 11)]].mean(axis=1)\n",
    "train_df['MA15'] = train_df[[f'RET_{i}' for i in range(1, 16)]].mean(axis=1)\n",
    "test_df['MA5'] = test_df[[f'RET_{i}' for i in range(1, 7)]].mean(axis=1)\n",
    "test_df['MA10'] = test_df[[f'RET_{i}' for i in range(1, 11)]].mean(axis=1)\n",
    "test_df['MA15'] = test_df[[f'RET_{i}' for i in range(1, 16)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13553ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "signe_of_return = LabelEncoder()\n",
    "train_df['RET'] = signe_of_return.fit_transform(train_df['RET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4961e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Default selection\n",
    "features = [x for x in train_df.columns if x not in [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0085439",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model,X,Y,kfold):\n",
    "    # Cross Validation to test and anticipate overfitting problem\n",
    "    scores1 = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
    "    scores2 = cross_val_score(model, X, Y, cv=kfold, scoring='precision')\n",
    "    scores3 = cross_val_score(model, X, Y, cv=kfold, scoring='recall')\n",
    "    # The mean score and standard deviation of the score estimate\n",
    "    print(\"Cross Validation Accuracy: %0.5f (+/- %0.2f)\" % (scores1.mean(), scores1.std()))\n",
    "    print(\"Cross Validation Precision: %0.5f (+/- %0.2f)\" % (scores2.mean(), scores2.std()))\n",
    "    print(\"Cross Validation Recall: %0.5f (+/- %0.2f)\" % (scores3.mean(), scores3.std()))\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(Y, y_pred, plot=True):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    auc_score = dict()\n",
    "    fpr, tpr, _ = roc_curve(Y, y_pred)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        plt.plot(fpr, tpr, color='blue',\n",
    "                 label='ROC curve (area = %0.2f)' % auc_score)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.show()\n",
    "    return fpr, tpr, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model,features,selection=False) : \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_  )\n",
    "    feature_importances = feature_importances.T\n",
    "    feature_importances.columns = [features]\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(13,12)})\n",
    "    fig = sns.barplot(data=feature_importances, orient='h', order=feature_importances.mean().sort_values(ascending=False).index)\n",
    "    fig.set(title = 'Feature importance', xlabel = 'features', ylabel = 'features_importance' )\n",
    "    \n",
    "    if selection: #Selection of features with min 2% of feature importance\n",
    "        n_features = feature_importances[feature_importances.loc[:,] > 0.02].dropna(axis='columns')\n",
    "        n_features = n_features.columns.get_level_values(0)    \n",
    "        print(\"Selected features\")\n",
    "        print(n_features)\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3999f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(model,X,Y,features, performCV=True,roc=False, printFeatureImportance=False):\n",
    "    \n",
    "    #Fitting the model on the data_set\n",
    "    model.fit(X[features],Y)\n",
    "        \n",
    "    #Predict training set:\n",
    "    predictions = model.predict(X[features])\n",
    "    predprob = model.predict_proba(X[features])[:,1]\n",
    "    \n",
    "    # Create and print confusion matrix    \n",
    "    cfm = confusion_matrix(Y,predictions)\n",
    "    print(\"\\nModel Confusion matrix\")\n",
    "    print(cfm)\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(Y.values, predictions))\n",
    "    \n",
    "    #Perform cross-validation: evaluate using 10-fold cross validation \n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    if performCV:\n",
    "        evaluation(model,X[features],Y,kfold)\n",
    "    if roc: \n",
    "        compute_roc(Y, predictions, plot=True)\n",
    "          \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feature_importance(model,features,selection=False) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3621f",
   "metadata": {},
   "source": [
    "## ML DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded555b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test set splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_df.loc[:, train_df.columns != 'RET'], train_df.RET, test_size=0.25, random_state =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23823ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decison tree baseline model\n",
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Decison tree baseline model\n",
    "modelfit(model,x_train, y_train, features,performCV=False)\n",
    "print(\"Accuracy on test set :{:.3f} \".format(model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f02d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Tunning Decision tree model  With Gridsearch\n",
    "print('Decision tree with Classifier')\n",
    "params={'max_depth': np.arange(2, 7),'criterion':['gini','entropy']}\n",
    "tree_estimator = tree.DecisionTreeClassifier()\n",
    "\n",
    "grid_tree = GridSearchCV(tree_estimator, params, cv=kfold, scoring=\"accuracy\",\n",
    "                         n_jobs=1,\n",
    "                         verbose=False)\n",
    "\n",
    "grid_tree.fit(x_train, y_train)\n",
    "best_est = grid_tree.best_estimator_\n",
    "print(best_est)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_tree.best_score_, grid_tree.best_params_))\n",
    "means = grid_tree.cv_results_['mean_test_score']\n",
    "stds = grid_tree.cv_results_['std_test_score']\n",
    "params = grid_tree.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f82bf9",
   "metadata": {},
   "source": [
    "**the best Hyperparameters for our Decision tree model using gridsearch Cv  is {'criterion': 'gini', 'max_depth': 6}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983164",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth = 6,criterion='gini')\n",
    "modelfit(model,x_train, y_train,features,printFeatureImportance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45e297",
   "metadata": {},
   "source": [
    "## features selection based on Feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b204b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(model,selection=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184defba",
   "metadata": {},
   "source": [
    "**we removed features with less than 2% of feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78735440",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = ['Mean', 'RET_1', 'RET_10','RET_15','RET_16', 'RET_17','RET_18', 'RET_19', 'RET_2', \n",
    "              'RET_20', 'RET_4', 'RET_7', 'RET_8','VOLUME_1', 'VOLUME_11', 'VOLUME_18', 'VOLUME_4']\n",
    "#New sets \n",
    "x_train_sl, x_test_sl, y_train_sl, y_test_sl = train_test_split(train_df.loc[:, train_df[n_features].columns], train_df.RET, random_state =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth = 6,criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eef4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting with train set\")\n",
    "modelfit(model,x_train_sl, y_train_sl,n_features,printFeatureImportance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75582707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting with test set\")\n",
    "modelfit(model,x_test_sl, y_test_sl,n_features,printFeatureImportance=False,roc= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on the test dataframe\n",
    "test_df = test_df[n_features]\n",
    "prediction = model.predict(test_df)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03786db",
   "metadata": {},
   "source": [
    "## ML GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52681cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test set splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_df.loc[:, train_df.columns != 'RET'], train_df.RET, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Gradient boosting model \n",
    "base_gbm = GradientBoostingClassifier(random_state=10)\n",
    "modelfit(base_gbm,x_train, y_train,features,roc=True,printFeatureImportance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9745e2a",
   "metadata": {},
   "source": [
    "**Tunning parameters with Gridsearch**\n",
    "** Baseline approch**\n",
    "   *Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "    min_samples_split = 500 : This should be ~0.5-1% of total values.\n",
    "    min_samples_leaf = 50 :  for preventing overfitting and again a small value.\n",
    "    max_depth = 8 : Should be chosen (5-8) based on the number of observations and predictors.\n",
    "    max_features = ‘sqrt’ : Its a general thumb-rule to start with square root.\n",
    "    subsample = 0.8 : commonly used used start value\n",
    "\n",
    "**we will choose all the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tuning n_estimators')\n",
    "params1 = {'n_estimators':range(30,81,10)}\n",
    "\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                       min_samples_split=500,\n",
    "                                       min_samples_leaf=50,\n",
    "                                       max_depth=8,\n",
    "                                       max_features='sqrt',\n",
    "                                       subsample=0.8,\n",
    "                                       random_state=10)\n",
    "\n",
    "grid_xgb1 = GridSearchCV(estimator,\n",
    "                  params1,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=1,\n",
    "                  verbose=False)\n",
    "grid_result=grid_xgb1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5983224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tuning max_depth and min_sample_split')\n",
    "params2 =  {'max_depth':range(5,16,2), 'min_samples_split':range(400,1001,200)}\n",
    "\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                       n_estimators = 80,\n",
    "                                       max_features='sqrt',\n",
    "                                       subsample=0.8,\n",
    "                                       random_state=10)\n",
    "\n",
    "grid_xgb2 = GridSearchCV(estimator,\n",
    "                  params2,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=True)\n",
    "\n",
    "grid_result=grid_xgb2.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d6928",
   "metadata": {},
   "source": [
    "tuning max_depth and min_sample_split\n",
    "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
    "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
    "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 134.9min finished\n",
    "Best: 0.540790 using {'max_depth': 15, 'min_samples_split': 400}\n",
    "0.532043 (0.000315) with: {'max_depth': 5, 'min_samples_split': 400}\n",
    "0.532069 (0.000926) with: {'max_depth': 5, 'min_samples_split': 600}\n",
    "0.532281 (0.000915) with: {'max_depth': 5, 'min_samples_split': 800}\n",
    "0.531996 (0.001207) with: {'max_depth': 5, 'min_samples_split': 1000}\n",
    "0.535426 (0.001870) with: {'max_depth': 7, 'min_samples_split': 400}\n",
    "0.534912 (0.000907) with: {'max_depth': 7, 'min_samples_split': 600}\n",
    "0.535163 (0.001501) with: {'max_depth': 7, 'min_samples_split': 800}\n",
    "0.534989 (0.001050) with: {'max_depth': 7, 'min_samples_split': 1000}\n",
    "0.537268 (0.001484) with: {'max_depth': 9, 'min_samples_split': 400}\n",
    "0.537225 (0.001056) with: {'max_depth': 9, 'min_samples_split': 600}\n",
    "0.536886 (0.002497) with: {'max_depth': 9, 'min_samples_split': 800}\n",
    "0.535931 (0.000559) with: {'max_depth': 9, 'min_samples_split': 1000}\n",
    "0.538961 (0.001785) with: {'max_depth': 11, 'min_samples_split': 400}\n",
    "0.538715 (0.001752) with: {'max_depth': 11, 'min_samples_split': 600}\n",
    "0.539016 (0.002292) with: {'max_depth': 11, 'min_samples_split': 800}\n",
    "0.537764 (0.002626) with: {'max_depth': 11, 'min_samples_split': 1000}\n",
    "0.540009 (0.000849) with: {'max_depth': 13, 'min_samples_split': 400}\n",
    "0.539254 (0.001786) with: {'max_depth': 13, 'min_samples_split': 600}\n",
    "0.538851 (0.001672) with: {'max_depth': 13, 'min_samples_split': 800}\n",
    "0.539967 (0.001591) with: {'max_depth': 13, 'min_samples_split': 1000}\n",
    "0.540790 (0.002216) with: {'max_depth': 15, 'min_samples_split': 400}\n",
    "0.539504 (0.001396) with: {'max_depth': 15, 'min_samples_split': 600}\n",
    "0.540633 (0.002234) with: {'max_depth': 15, 'min_samples_split': 800}\n",
    "0.539700 (0.002217) with: {'max_depth': 15, 'min_samples_split': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d352f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best parameter is give by Best: 0.540790 using {'max_depth': 15, 'min_samples_split': 400}\n",
    "print('tuning num_sample_split and min_sample_split')\n",
    "params3 =  {'min_samples_leaf':range(40,70,10), 'min_samples_split':range(400,1001,200)}\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                       n_estimators = 80,\n",
    "                                       max_depth=15,\n",
    "                                       max_features='sqrt',\n",
    "                                       subsample=0.8,\n",
    "                                       random_state=10)\n",
    "grid_xgb3 = GridSearchCV(estimator,\n",
    "                  params3,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=True)\n",
    "grid_result=grid_xgb3.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fitting of the Grid search best estimator\n",
    "modelfit(grid_xgb3.best_estimator_,x_train, y_train,features,roc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eae144",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(grid_xgb3.best_estimator_,x_test, y_test,features,roc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tuning max_features')\n",
    "params4 =  {'max_features':range(7,20,2)}\n",
    "\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                       n_estimators = 80,\n",
    "                                       max_depth=15,\n",
    "                                        min_samples_split=400, \n",
    "                                       min_samples_leaf=40, \n",
    "                                       subsample=0.8,\n",
    "                                       random_state=10)\n",
    "grid_xgb4 = GridSearchCV(estimator,\n",
    "                  params4,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=1,\n",
    "                  verbose=True)\n",
    "grid_result=grid_xgb4.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds , params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tunned = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                       n_estimators = 80,\n",
    "                                       max_depth=19,\n",
    "                                        min_samples_split=400, \n",
    "                                       min_samples_leaf=40, \n",
    "                                       subsample=0.8,\n",
    "                                       random_state=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Cross validation and prediction on the train and the test set\n",
    "modelfit(xgb_tunned,x_train, y_train,features,performCV=True,roc=False,printFeatureImportance=True)\n",
    "modelfit(xgb_tunned,x_test, y_test,features, performCV=True,roc=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
